---
title: "Homework 3 - BSTA 522"
author: "Matthew Hoctor"
date: "1/21/2022"
output:
  html_document:
    number_sections: no
    theme: lumen
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: no
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# library(dplyr)
# library(readxl)
library(tidyverse)
library(ggplot2)
# library(CarletonStats)
# library(pwr)
# library(BSDA)
# library(exact2x2)
# library(car)
# library(dvmisc)
# library(emmeans)
# library(gridExtra)
# library(DescTools)
# library(DiagrammeR)
# library(nlme)
# library(doBy)
# library(geepack)
library(ISLR2)
library(psych)
library(MASS)
library(caret)  #required for confusionMatrix function
library(rje)
library(class)  #required for the knn function
library(e1071)  #required for the naiveBayes function
```

# Part 2

## Tarran and Ghahramani Summary

 * Tarran and Ghahramani introduce the idea of weak AI and strong AI;$^1$ weak AI is defined as specialized AI which does a particular set of related tasks very well, wheras strong AI is defined as AI in possession of 'general human intelligence', which according to Nick Bostrom entails that the AI would have 'common sense and an effective ability to learn, reason and plan to meet complex information-processing challenges'.
 * Historical examples of weak AI are given; these include Logic Expert, which proved many of the theorems of Bertrand Russell's _Principia Mathmetica_, and General Problem Solver, which solved a limited set of puzzles.  The limiting factor of these AIs was their reliance on search tree algorithms, which become computationally burdensome for more general problems.
 * Expert systems, in which AI is give subject-matter expertise in order to solve a specific real-world problem, is described.  According to Peter Norvig, emulating human expertise in this way has limited applicability because human thought processes are not described sufficiently as to be emulated, and also because a determinist logical approach was taken as opposed to a probabilist approach, which better reflects reality.
 * Judea Pearl's concept of Bayesian Networks was developed, in part, to better implement a probabilistic approach in AI.  Bayesian Networks are DAGs, in which vertices represent variables, and edges represent causation, with the strength of the causation quantified by the conditional probability of the subsequent vertex given a certain value of the previous vertex.
 * Maximum Likelihood Estimation (MLE), in which parameters of an assumed model are found which maximize the likelihood function, which is the joint conditional probability of observing the observed values given certain parameter values of the assumed model.  This is the premise of Cathy O'Neil's book.$^2$

## Olhede and Wolfe Summary

 * Olhede and Wolfe sees to answer the question of legal liability for incorrect algorithmic decision making by describing a tradeoff between four algorithmic traits;$^3$ these include computational complexity, typical performance, stability, and robustness.  It is not clear who is legally responsible for such inevitable tradeoffs in algorithmic design.  Furthermore, the article points out that unfairness and discrimination in training datasets will inevitably produce an algorithm which reproduces the unfairness in the dataset.  
 
## Tarran Summary

 * Tarran explores possible explanations for the observation that customers prefer items with more reviews even if they is less favourably rated.$^4$  Star ratings provide subjective information, as they do not mean the same things to each consumer or rater; furthermore amazon has started calculating a weighted mean star rating based on rating date, verified status of the rater, and helpfulness of the rater.  Median rating is put forward as a statistical solution to this issue. 

# Part 3

## 4.6

### a

We are given:

$$\mathbb{logit}[\hat{P}(Y = 1|X)] = \hat{\beta_0} + \hat{\beta_1} X_1 + \hat{\beta_2} X_2$$

Where the logit function is the natural log of the odds.  Expit is the inverse function of logit, therefore:

$$\hat{P}(Y = 1|X) = \mathbb{expit}[\hat{\beta_0} + \hat{\beta_1} X_1 + \hat{\beta_2} X_2]$$

With the given values of $\hat{\beta_i}$, we can evaluate the probability of a student who studies for 40 h and had an undergraduate GPA of 3.5 getting an A in the class:

```{r}
expit(-6 + 0.05*40  + 1*3.5)
```

### b

If we are given that the student's undergraduate GPA remains 3.5 ($X_2 = 3.5$), we can find the number of hours ($X_1$) required for the probability of receiving an A to reach 50%; i.e. for $\hat{P}(Y = 1|X) = 0.50$:

$$
\begin{split}
0.50 &=  \mathbb{expit}[\hat{\beta_0} + \hat{\beta_1} X_1 + \hat{\beta_2} X_2]\\
\mathbb{logit}[0.50] &=  \hat{\beta_0} + \hat{\beta_1} X_1 + \hat{\beta_2} X_2\\
X_1 &= \frac{\mathbb{logit}[0.50] - \hat{\beta_0} - \hat{\beta_2} X_2}{\hat{\beta_1}}
\end{split}
$$

We can now evaluate this quantity:

```{r}
(logit(0.5) -(-6) -(1*3.5))/0.05
```

We can see that 10 more hours of study would bring this student's probability of receiving an A up to 50%.

## 4.7

This conditional probability can be expressed using Bayes' Thorem:

$$P(Y= yes | X = 4) =  \frac{\pi_{yes} * f_{yes}(X=4)}{\pi_{yes} * f_{yes}(X=4) + \pi_{no} * f_{no}(X=4)}$$

Here $\pi_{yes} = 0.8$ is the probability of issuing dividends, $\pi_{no} = 0.2$ is the probability of not issuing dividends, and $f$ is the normal distribution with $\mu_{yes} = 10$, $\mu_{no} = 0$, and $\sigma^2 = 36$.  This probability can be evaluated in R with the following code:

```{r}
(0.8*dnorm(4,mean = 10, sd = 6))/(0.8*dnorm(4,mean = 10, sd = 6) + 0.2*dnorm(4,mean = 0, sd = 6))
```

There is roughly a 75% probability that a company with 4% profitability is the past year will issue dividends.

## 4.13

```{r}
weekly <- Weekly
```

### a

Here are summary statistics for each variable in the dataset:

```{r}
summary(weekly)
```

We can create a long-form dataset for easier plotting:

```{r}
weekly_long <- pivot_longer(weekly,
                            cols = c("Lag1", "Lag2", "Lag3", "Lag4", "Lag5"),
                            names_to = "Lag",
                            values_to = "Return")
```

We can now plot the boxplots:

```{r}
ggplot(weekly_long,
       aes(x = Lag, y = Return)) +
  geom_boxplot()
```

Here we not that Lag1-Lag5 are percentage return for the previous 1-5 weeks respectively.  We can now make a scatterplot matrix:

```{r}
pairs.panels(
  weekly,
  stars = TRUE
  )
```

We can see that there is some correlation among lag variables, and a large correlation between trading volume and year.

### b

```{r}
weekly.glm0 <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                  data = weekly,
                  family = binomial
                  )
summary(weekly.glm0)
```

Based on these results, the lag at two weeks (Lag2) seems significant.

### c

Creating the confusion matrix in two different ways:

```{r}
yhat0=predict(weekly.glm0,type="response")
table(yhat0>=0.5,weekly$Direction)

confusionMatrix(data=factor(yhat0>=0.5),
                reference=factor(weekly$Direction=="Up"),
                positive="TRUE")
```

The confusion matrix shows correct predictions along the diagonal, and incorrect predictions along the off-diagonals.  Other information can be derived from the table, including PPV, NPV, sensitivity, specificity, etc.

### d

We will specify the training and testing datasets:

```{r}
train <- (weekly$Year < 2009)

train.subset <- subset(weekly, Year<2009)
test.subset <- subset(weekly, Year>2008)
dim(train.subset)
dim(test.subset)
```

Perform the logistic regression:

```{r}
weekly.glm1 <- glm(Direction ~ Lag2 + Volume,
                  data = weekly,
                  family = binomial,
                  subset = train
                  )
summary(weekly.glm1)
```

We can use this model to compute a confusion matrix for the years 2009 & 2010:

```{r}
yhat1=predict(weekly.glm1,
              weekly,
              type="response")
table(yhat1[!train]>=0.5,test.subset$Direction)

confusionMatrix(data=factor(yhat1[!train]>=0.5),
                reference=factor(test.subset$Direction=="Up"),
                positive="TRUE")
```

### e

Repeating part d with LDA.

```{r}
weekly.lda0 <- lda(Direction ~ Lag2 + Volume,
                  data = weekly,
                  subset = train
                  )
summary(weekly.lda0)
```

We can use this model to compute a confusion matrix and fraction of correct predictions (reported as Accuracy from the confusionMatrix function output) for the years 2009 & 2010:

```{r}
yhat2=predict(weekly.lda0,
              weekly,
              type="response")

posterior2 <- data.frame(yhat2$posterior)

table(posterior2$Up[!train]>=0.5,test.subset$Direction)

confusionMatrix(data=factor(posterior2$Up[!train]>=0.5),
                reference=factor(test.subset$Direction=="Up"),
                positive="TRUE")
```

After converting the posterior values from the data frame output from predicting a .lda object, the prior methods can be used for creation of the confusion matrix.

### f

Repeat d with QDA.  Note that adding trade volume as an additional predictor improves results

```{r}
weekly.qda0 <- qda(Direction ~ Lag2 + Volume,
                  data = weekly,
                  subset = train
                  )
summary(weekly.qda0)
```

We can use this model to compute a confusion matrix and fraction of correct predictions (reported as Accuracy from the confusionMatrix function output) for the years 2009 & 2010:

```{r}
yhat3=predict(weekly.qda0,
              weekly,
              type="response")

posterior3 <- data.frame(yhat3$posterior)

table(posterior3$Up[!train]>=0.5, test.subset$Direction)

confusionMatrix(data=factor(posterior3$Up[!train]>=0.5),
                reference=factor(test.subset$Direction=="Up"),
                positive="TRUE")
```

### g

Repeat d with KNN with K = 1.  Borrowing some of the variable conventions from the code in the text:

```{r}
train.x <- as.data.frame(train.subset$Lag2)
test.x <- as.data.frame(test.subset$Lag2)
train.Direction <- as.factor(train.subset$Direction)

set.seed(1237)
knn.pred1 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 1)

table(knn.pred1, test.subset$Direction)

confusionMatrix(data=knn.pred1,
                reference=test.subset$Direction,
                positive=NULL)
```

### h

Repeat d using naive Bayes.

```{r}
weekly.nb0 <- naiveBayes(Direction ~ Lag2 + Volume,
                  data = weekly,
                  subset = train
                  )
weekly.nb0
summary(weekly.nb0)
```

We can calculate the confusion matrix for this approach:

```{r}
yhat5=predict(weekly.nb0,
              weekly)
table(yhat5[!train],test.subset$Direction)

# y5 <- factor(yhat5[-train]=="Up")
confusionMatrix(data=yhat5[!train],
                reference=factor(test.subset$Direction),
                positive=NULL)
```

### i

Which of these methods appears to provide the best results on this data?

Based on accuracy, LDA and logistic regression are roughly equally as good, with an accuracy of 0.54.  If the consequences of false positives and false negatives are not assumed to be equal then a more in-deapth comparison looking at sensitivity and specificity would be required.

### j

Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.

#### KNN

We can first look at different values of K for KNN:

```{r}
set.seed(1237)
knn.pred2 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 2)
cat('K=2')
confusionMatrix(data=knn.pred2,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred3 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 3)
cat('K=3')
confusionMatrix(data=knn.pred3,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred4 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 4)
cat('K=4')
confusionMatrix(data=knn.pred4,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred5 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 5)
cat('K=5')
confusionMatrix(data=knn.pred5,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred6 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 6)
cat('K=6')
confusionMatrix(data=knn.pred6,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred7 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 7)
cat('K=7')
confusionMatrix(data=knn.pred7,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred8 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 8)
cat('K=8')
confusionMatrix(data=knn.pred8,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred9 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 9)
cat('K=9')
confusionMatrix(data=knn.pred9,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred10 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 10)
cat('K=10')
confusionMatrix(data=knn.pred10,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred11 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 11)
cat('K=11')
confusionMatrix(data=knn.pred11,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred12 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 12)
cat('K=12')
confusionMatrix(data=knn.pred12,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred13 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 13)
cat('K=13')
confusionMatrix(data=knn.pred13,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred14 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 14)
cat('K=14')
confusionMatrix(data=knn.pred14,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred15 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 15)
cat('K=15')
confusionMatrix(data=knn.pred15,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred20 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 20)
cat('K=20')
confusionMatrix(data=knn.pred20,
                reference=test.subset$Direction,
                positive=NULL)$overall
knn.pred25 <- knn(train = train.x, 
                test = test.x,
                cl = train.Direction, 
                k = 25)
cat('K=25')
confusionMatrix(data=knn.pred25,
                reference=test.subset$Direction,
                positive=NULL)$overall
```

Accuracy seems to improve with increasing values of K, until around k=10.  With an accuracy of 0.596 achieved at K=12, this is the best result so far.  However, KNN may benefit from predictors whic were not significant in the logistic regression:

```{r}
set.seed(1237)
train.x1 <- cbind(train.subset$Lag1, train.subset$Lag2)
test.x1 <- cbind(test.subset$Lag1, test.subset$Lag2)
train.Direction <- as.factor(train.subset$Direction)

knn.pred.x1 <- knn(train = train.x1, 
                test = test.x1,
                cl = train.Direction, 
                k = 12)
cat('addLag1')
confusionMatrix(data=knn.pred.x1,
                reference=test.subset$Direction,
                positive=NULL)$overall

train.x3 <- cbind(train.subset$Lag3, train.subset$Lag2)
test.x3 <- cbind(test.subset$Lag3, test.subset$Lag2)
train.Direction <- as.factor(train.subset$Direction)

knn.pred.x3 <- knn(train = train.x3, 
                test = test.x3,
                cl = train.Direction, 
                k = 12)
cat('addLag3')
confusionMatrix(data=knn.pred.x3,
                reference=test.subset$Direction,
                positive=NULL)$overall

train.x4 <- cbind(train.subset$Lag4, train.subset$Lag2)
test.x4 <- cbind(test.subset$Lag4, test.subset$Lag2)
train.Direction <- as.factor(train.subset$Direction)

knn.pred.x4 <- knn(train = train.x4, 
                test = test.x4,
                cl = train.Direction, 
                k = 12)
cat('addLag4')
confusionMatrix(data=knn.pred.x4,
                reference=test.subset$Direction,
                positive=NULL)$overall

train.x5 <- cbind(train.subset$Lag5, train.subset$Lag2)
test.x5 <- cbind(test.subset$Lag5, test.subset$Lag2)
train.Direction <- as.factor(train.subset$Direction)

knn.pred.x5 <- knn(train = train.x5, 
                test = test.x5,
                cl = train.Direction, 
                k = 12)
cat('addLag5')
confusionMatrix(data=knn.pred.x5,
                reference=test.subset$Direction,
                positive=NULL)$overall

```

None of these additional variables improve accuracy.  We can proceed to LDA.

#### LDA

Factors which were not found significant in logistic regression may improve LDA performance.

```{r}
weekly.lda1 <- lda(Direction ~ Lag1 + Lag2 + Volume,
                  data = weekly,
                  subset = train
                  )
yhat.lda1=predict(weekly.lda1,
              weekly,
              type="response")
posterior.lda1 <- data.frame(yhat.lda1$posterior)
cat('Add Lag1')
confusionMatrix(data=factor(posterior.lda1$Up[!train]>=0.5),
                reference=factor(test.subset$Direction=="Up"),
                positive="TRUE")$overall

weekly.lda3 <- lda(Direction ~ Lag3 + Lag2 + Volume,
                  data = weekly,
                  subset = train
                  )
yhat.lda3=predict(weekly.lda3,
              weekly,
              type="response")
posterior.lda3 <- data.frame(yhat.lda3$posterior)
cat('Add Lag3')
confusionMatrix(data=factor(posterior.lda3$Up[!train]>=0.5),
                reference=factor(test.subset$Direction=="Up"),
                positive="TRUE")$overall

weekly.lda4 <- lda(Direction ~ Lag4 + Lag2 + Volume,
                  data = weekly,
                  subset = train
                  )
yhat.lda4=predict(weekly.lda4,
              weekly,
              type="response")
posterior.lda4 <- data.frame(yhat.lda4$posterior)
cat('Add Lag4')
confusionMatrix(data=factor(posterior.lda4$Up[!train]>=0.5),
                reference=factor(test.subset$Direction=="Up"),
                positive="TRUE")$overall

weekly.lda5 <- lda(Direction ~ Lag5 + Lag2 + Volume,
                  data = weekly,
                  subset = train
                  )
yhat.lda5=predict(weekly.lda5,
              weekly,
              type="response")
posterior.lda5 <- data.frame(yhat.lda5$posterior)
cat('Add Lag5')
confusionMatrix(data=factor(posterior.lda5$Up[!train]>=0.5),
                reference=factor(test.subset$Direction=="Up"),
                positive="TRUE")$overall
```

Adding Lag3 to Lag2 marginally improves accuracy from 0.538 to 0.567.  This is still not as good as what was acheived with KNN.

#### QDA

We can repeat the above search for variables with the QDA method:

```{r}
weekly.qda1 <- qda(Direction ~ Lag1 + Lag2 + Volume,
                  data = weekly,
                  subset = train
                  )
yhat.qda1=predict(weekly.qda1,
              weekly,
              type="response")
posterior.qda1 <- data.frame(yhat.qda1$posterior)
cat('Add Lag1')
confusionMatrix(data=factor(posterior.qda1$Up[!train]>=0.5),
                reference=factor(test.subset$Direction=="Up"),
                positive="TRUE")

weekly.qda3 <- qda(Direction ~ Lag3 + Lag2 + Volume,
                  data = weekly,
                  subset = train
                  )
yhat.qda3=predict(weekly.qda3,
              weekly,
              type="response")
posterior.qda3 <- data.frame(yhat.qda3$posterior)
cat('Add Lag3')
confusionMatrix(data=factor(posterior.qda3$Up[!train]>=0.5),
                reference=factor(test.subset$Direction=="Up"),
                positive="TRUE")

weekly.qda4 <- qda(Direction ~ Lag4 + Lag2 + Volume,
                  data = weekly,
                  subset = train
                  )
yhat.qda4=predict(weekly.qda4,
              weekly,
              type="response")
posterior.qda4 <- data.frame(yhat.qda4$posterior)
cat('Add Lag4')
confusionMatrix(data=factor(posterior.qda4$Up[!train]>=0.5),
                reference=factor(test.subset$Direction=="Up"),
                positive="TRUE")

weekly.qda5 <- qda(Direction ~ Lag5 + Lag2 + Volume,
                  data = weekly,
                  subset = train
                  )
yhat.qda5=predict(weekly.qda5,
              weekly,
              type="response")
posterior.qda5 <- data.frame(yhat.qda5$posterior)
cat('Add Lag5')
confusionMatrix(data=factor(posterior.qda5$Up[!train]>=0.5),
                reference=factor(test.subset$Direction=="Up"),
                positive="TRUE")
```

Accuracy improves marginally from 0.47 to 0.48 when Lag5 is added.

#### Conclusion

The best results were found with KNN using K=12.  Further improvements are likely possible.

# Session Info

```{r}
sessionInfo()
```

# References

1. Tarran B, Ghahramani Z. How machines learned to think statistically. Significance. 2015;12(1):8-15. doi:10.1111/j.1740-9713.2015.00796.x
2. Oâ€™Neil C. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Reprint edition. Crown; 2016.
3. Olhede S, Wolfe P. When algorithms go wrong, who is liable? Significance. 2017;14(6):8-9. doi:10.1111/j.1740-9713.2017.01085.x
4. Tarran B. Thinking statistically about online customer reviews. Significance. 2017;14(6):4-5. doi:10.1111/j.1740-9713.2017.01083.x
5. James G, Witten D, Hastie T, Tibshirani R. An Introduction to Statistical Learning: With Applications in R. 1st ed. 2013, Corr. 7th printing 2017 edition. Springer; 2013.

